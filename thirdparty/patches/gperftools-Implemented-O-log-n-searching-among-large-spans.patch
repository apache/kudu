From 071cadad7a752c5678c18174a74b82858404f762 Mon Sep 17 00:00:00 2001
From: Aliaksey Kandratsenka <alkondratenko@gmail.com>
Date: Sat, 3 Jun 2017 15:31:06 -0700
Subject: [PATCH 1/1] Implemented O(log n) searching among large spans

This is implemented via std::set with custom STL allocator that
delegates to PageHeapAllocator. Free large spans are not linked
together via linked list, but inserted into std::set. Spans also store
iterators to std::set positions pointing to them. So that removing
span from set is fast too.

Patch implemented by Aliaksey Kandratsenka and Todd Lipcon based on
earlier research and experimentation by James Golick.

Addresses issue #535
---
 src/page_heap.cc               |  152 +++++++++++++++++++++++++---------------
 src/page_heap.h                |   18 ++++--
 src/page_heap_allocator.h      |   64 +++++++++++++++++
 src/span.h                     |   61 ++++++++++++++++-
 src/tests/tcmalloc_unittest.cc |    2 +-
 5 files changed, 234 insertions(+), 63 deletions(-)

diff --git a/src/page_heap.cc b/src/page_heap.cc
index 0dda58f..712732a 100644
--- a/src/page_heap.cc
+++ b/src/page_heap.cc
@@ -69,8 +69,6 @@ PageHeap::PageHeap()
       release_index_(kMaxPages),
       aggressive_decommit_(false) {
   COMPILE_ASSERT(kClassSizesMax <= (1 << PageMapCache::kValuebits), valuebits);
-  DLL_Init(&large_.normal);
-  DLL_Init(&large_.returned);
   for (int i = 0; i < kMaxPages; i++) {
     DLL_Init(&free_[i].normal);
     DLL_Init(&free_[i].returned);
@@ -168,45 +166,38 @@ Span* PageHeap::New(Length n) {
 }
 
 Span* PageHeap::AllocLarge(Length n) {
-  // find the best span (closest to n in size).
-  // The following loops implements address-ordered best-fit.
   Span *best = NULL;
+  Span *best_normal = NULL;
 
-  // Search through normal list
-  for (Span* span = large_.normal.next;
-       span != &large_.normal;
-       span = span->next) {
-    if (span->length >= n) {
-      if ((best == NULL)
-          || (span->length < best->length)
-          || ((span->length == best->length) && (span->start < best->start))) {
-        best = span;
-        ASSERT(best->location == Span::ON_NORMAL_FREELIST);
-      }
-    }
-  }
+  // Create a Span to use as an upper bound.
+  Span bound;
+  bound.start = 0;
+  bound.length = n;
 
-  Span *bestNormal = best;
+  // First search the NORMAL spans..
+  SpanSet::iterator place = large_normal_.upper_bound(SpanPtrWithLength(&bound));
+  if (place != large_normal_.end()) {
+    best = place->span;
+    best_normal = best;
+    ASSERT(best->location == Span::ON_NORMAL_FREELIST);
+  }
 
-  // Search through released list in case it has a better fit
-  for (Span* span = large_.returned.next;
-       span != &large_.returned;
-       span = span->next) {
-    if (span->length >= n) {
-      if ((best == NULL)
-          || (span->length < best->length)
-          || ((span->length == best->length) && (span->start < best->start))) {
-        best = span;
-        ASSERT(best->location == Span::ON_RETURNED_FREELIST);
-      }
-    }
+  // Try to find better fit from RETURNED spans.
+  place = large_returned_.upper_bound(SpanPtrWithLength(&bound));
+  if (place != large_returned_.end()) {
+    Span *c = place->span;
+    ASSERT(c->location == Span::ON_RETURNED_FREELIST);
+    if (best_normal == NULL
+        || c->length < best->length
+        || (c->length == best->length && c->start < best->start))
+      best = place->span;
   }
 
-  if (best == bestNormal) {
+  if (best == best_normal) {
     return best == NULL ? NULL : Carve(best, n);
   }
 
-  // best comes from returned list.
+  // best comes from RETURNED set.
 
   if (EnsureLimit(n, false)) {
     return Carve(best, n);
@@ -214,13 +205,13 @@ Span* PageHeap::AllocLarge(Length n) {
 
   if (EnsureLimit(n, true)) {
     // best could have been destroyed by coalescing.
-    // bestNormal is not a best-fit, and it could be destroyed as well.
+    // best_normal is not a best-fit, and it could be destroyed as well.
     // We retry, the limit is already ensured:
     return AllocLarge(n);
   }
 
-  // If bestNormal existed, EnsureLimit would succeeded:
-  ASSERT(bestNormal == NULL);
+  // If best_normal existed, EnsureLimit would succeeded:
+  ASSERT(best_normal == NULL);
   // We are not allowed to take best from returned list.
   return NULL;
 }
@@ -406,12 +397,27 @@ void PageHeap::MergeIntoFreeList(Span* span) {
 
 void PageHeap::PrependToFreeList(Span* span) {
   ASSERT(span->location != Span::IN_USE);
-  SpanList* list = (span->length < kMaxPages) ? &free_[span->length] : &large_;
-  if (span->location == Span::ON_NORMAL_FREELIST) {
+  if (span->location == Span::ON_NORMAL_FREELIST)
     stats_.free_bytes += (span->length << kPageShift);
+  else
+    stats_.unmapped_bytes += (span->length << kPageShift);
+
+  if (span->length >= kMaxPages) {
+    SpanSet *set = &large_normal_;
+    if (span->location == Span::ON_RETURNED_FREELIST)
+      set = &large_returned_;
+    std::pair<SpanSet::iterator, bool> p =
+        set->insert(SpanPtrWithLength(span));
+    ASSERT(p.second); // We never have duplicates since span->start is unique.
+    span->rev_ptr.set_iterator(p.first);
+    ASSERT(span->rev_ptr.get_iterator()->span == span);
+    return;
+  }
+
+  SpanList* list = &free_[span->length];
+  if (span->location == Span::ON_NORMAL_FREELIST) {
     DLL_Prepend(&list->normal, span);
   } else {
-    stats_.unmapped_bytes += (span->length << kPageShift);
     DLL_Prepend(&list->returned, span);
   }
 }
@@ -423,7 +429,16 @@ void PageHeap::RemoveFromFreeList(Span* span) {
   } else {
     stats_.unmapped_bytes -= (span->length << kPageShift);
   }
-  DLL_Remove(span);
+  if (span->length >= kMaxPages) {
+    SpanSet *set = &large_normal_;
+    if (span->location == Span::ON_RETURNED_FREELIST)
+      set = &large_returned_;
+    ASSERT(span->rev_ptr.get_iterator()->span == span);
+    ASSERT(set->find(SpanPtrWithLength(span)) == span->rev_ptr.get_iterator());
+    set->erase(span->rev_ptr.get_iterator());
+  } else {
+    DLL_Remove(span);
+  }
 }
 
 void PageHeap::IncrementalScavenge(Length n) {
@@ -459,8 +474,7 @@ void PageHeap::IncrementalScavenge(Length n) {
   }
 }
 
-Length PageHeap::ReleaseLastNormalSpan(SpanList* slist) {
-  Span* s = slist->normal.prev;
+Length PageHeap::ReleaseSpan(Span* s) {
   ASSERT(s->location == Span::ON_NORMAL_FREELIST);
 
   if (DecommitSpan(s)) {
@@ -477,21 +491,35 @@ Length PageHeap::ReleaseLastNormalSpan(SpanList* slist) {
 Length PageHeap::ReleaseAtLeastNPages(Length num_pages) {
   Length released_pages = 0;
 
-  // Round robin through the lists of free spans, releasing the last
-  // span in each list.  Stop after releasing at least num_pages
+  // Round robin through the lists of free spans, releasing a
+  // span from each list.  Stop after releasing at least num_pages
   // or when there is nothing more to release.
   while (released_pages < num_pages && stats_.free_bytes > 0) {
     for (int i = 0; i < kMaxPages+1 && released_pages < num_pages;
          i++, release_index_++) {
+      Span *s;
       if (release_index_ > kMaxPages) release_index_ = 0;
-      SpanList* slist = (release_index_ == kMaxPages) ?
-          &large_ : &free_[release_index_];
-      if (!DLL_IsEmpty(&slist->normal)) {
-        Length released_len = ReleaseLastNormalSpan(slist);
-        // Some systems do not support release
-        if (released_len == 0) return released_pages;
-        released_pages += released_len;
+
+      if (release_index_ == kMaxPages) {
+        if (large_normal_.empty()) {
+          continue;
+        }
+        s = (large_normal_.begin())->span;
+      } else {
+        SpanList* slist = &free_[release_index_];
+        if (DLL_IsEmpty(&slist->normal)) {
+          continue;
+        }
+        s = slist->normal.prev;
       }
+      // TODO(todd) if the remaining number of pages to release
+      // is significantly smaller than s->length, and s is on the
+      // large freelist, should we carve s instead of releasing?
+      // the whole thing?
+      Length released_len = ReleaseSpan(s);
+      // Some systems do not support release
+      if (released_len == 0) return released_pages;
+      released_pages += released_len;
     }
   }
   return released_pages;
@@ -544,12 +572,12 @@ void PageHeap::GetLargeSpanStats(LargeSpanStats* result) {
   result->spans = 0;
   result->normal_pages = 0;
   result->returned_pages = 0;
-  for (Span* s = large_.normal.next; s != &large_.normal; s = s->next) {
-    result->normal_pages += s->length;;
+  for (SpanSet::iterator it = large_normal_.begin(); it != large_normal_.end(); ++it) {
+    result->normal_pages += it->length;
     result->spans++;
   }
-  for (Span* s = large_.returned.next; s != &large_.returned; s = s->next) {
-    result->returned_pages += s->length;
+  for (SpanSet::iterator it = large_returned_.begin(); it != large_returned_.end(); ++it) {
+    result->returned_pages += it->length;
     result->spans++;
   }
 }
@@ -664,8 +692,8 @@ bool PageHeap::Check() {
 
 bool PageHeap::CheckExpensive() {
   bool result = Check();
-  CheckList(&large_.normal, kMaxPages, 1000000000, Span::ON_NORMAL_FREELIST);
-  CheckList(&large_.returned, kMaxPages, 1000000000, Span::ON_RETURNED_FREELIST);
+  CheckSet(&large_normal_, kMaxPages, Span::ON_NORMAL_FREELIST);
+  CheckSet(&large_returned_, kMaxPages, Span::ON_RETURNED_FREELIST);
   for (Length s = 1; s < kMaxPages; s++) {
     CheckList(&free_[s].normal, s, s, Span::ON_NORMAL_FREELIST);
     CheckList(&free_[s].returned, s, s, Span::ON_RETURNED_FREELIST);
@@ -685,4 +713,16 @@ bool PageHeap::CheckList(Span* list, Length min_pages, Length max_pages,
   return true;
 }
 
+bool PageHeap::CheckSet(SpanSet* spanset, Length min_pages,int freelist) {
+  for (SpanSet::iterator it = spanset->begin(); it != spanset->end(); ++it) {
+    Span* s = it->span;
+    CHECK_CONDITION(s->length == it->length);
+    CHECK_CONDITION(s->location == freelist);  // NORMAL or RETURNED
+    CHECK_CONDITION(s->length >= min_pages);
+    CHECK_CONDITION(GetDescriptor(s->start) == s);
+    CHECK_CONDITION(GetDescriptor(s->start+s->length-1) == s);
+  }
+  return true;
+}
+
 }  // namespace tcmalloc
diff --git a/src/page_heap.h b/src/page_heap.h
index b688512..34f18bb 100644
--- a/src/page_heap.h
+++ b/src/page_heap.h
@@ -196,6 +196,7 @@ class PERFTOOLS_DLL_DECL PageHeap {
   bool CheckExpensive();
   bool CheckList(Span* list, Length min_pages, Length max_pages,
                  int freelist);  // ON_NORMAL_FREELIST or ON_RETURNED_FREELIST
+  bool CheckSet(SpanSet *s, Length min_pages, int freelist);
 
   // Try to release at least num_pages for reuse by the OS.  Returns
   // the actual number of pages released, which may be less than
@@ -264,8 +265,12 @@ class PERFTOOLS_DLL_DECL PageHeap {
     Span        returned;
   };
 
-  // List of free spans of length >= kMaxPages
-  SpanList large_;
+  // Sets of spans with length >= kMaxPages.
+  //
+  // Rather than using a linked list, we use sets here for efficient
+  // best-fit search.
+  SpanSet large_normal_;
+  SpanSet large_returned_;
 
   // Array mapping from span length to a doubly linked list of free spans
   SpanList free_[kMaxPages];
@@ -317,9 +322,12 @@ class PERFTOOLS_DLL_DECL PageHeap {
   // IncrementalScavenge(n) is called whenever n pages are freed.
   void IncrementalScavenge(Length n);
 
-  // Release the last span on the normal portion of this list.
-  // Return the length of that span or zero if release failed.
-  Length ReleaseLastNormalSpan(SpanList* slist);
+  // Attempts to decommit 's' and move it to the returned freelist.
+  //
+  // Returns the length of the Span or zero if release failed.
+  //
+  // REQUIRES: 's' must be on the NORMAL freelist.
+  Length ReleaseSpan(Span *s);
 
   // Checks if we are allowed to take more memory from the system.
   // If limit is reached and allowRelease is true, tries to release
diff --git a/src/page_heap_allocator.h b/src/page_heap_allocator.h
index 892d1c1..440b8ad 100644
--- a/src/page_heap_allocator.h
+++ b/src/page_heap_allocator.h
@@ -109,6 +109,70 @@ class PageHeapAllocator {
   int inuse_;
 };
 
+// STL-compatible allocator which forwards allocations to a PageHeapAllocator.
+//
+// Like PageHeapAllocator, this requires external synchronization. To avoid multiple
+// separate STLPageHeapAllocator<T> from sharing the same underlying PageHeapAllocator<T>,
+// the |LockingTag| template argument should be used. Template instantiations with
+// different locking tags can safely be used concurrently.
+template <typename T, class LockingTag>
+class STLPageHeapAllocator {
+ public:
+  typedef size_t     size_type;
+  typedef ptrdiff_t  difference_type;
+  typedef T*         pointer;
+  typedef const T*   const_pointer;
+  typedef T&         reference;
+  typedef const T&   const_reference;
+  typedef T          value_type;
+
+  template <class T1> struct rebind {
+    typedef STLPageHeapAllocator<T1, LockingTag> other;
+  };
+
+  STLPageHeapAllocator() { }
+  STLPageHeapAllocator(const STLPageHeapAllocator&) { }
+  template <class T1> STLPageHeapAllocator(const STLPageHeapAllocator<T1, LockingTag>&) { }
+  ~STLPageHeapAllocator() { }
+
+  pointer address(reference x) const { return &x; }
+  const_pointer address(const_reference x) const { return &x; }
+
+  size_type max_size() const { return size_t(-1) / sizeof(T); }
+
+  void construct(pointer p, const T& val) { ::new(p) T(val); }
+  void construct(pointer p) { ::new(p) T(); }
+  void destroy(pointer p) { p->~T(); }
+
+  // There's no state, so these allocators are always equal
+  bool operator==(const STLPageHeapAllocator&) const { return true; }
+
+  pointer allocate(size_type n, const void* = 0) {
+    if (!underlying_.initialized) {
+      underlying_.allocator.Init();
+      underlying_.initialized = true;
+    }
+
+    CHECK_CONDITION(n == 1);
+    return underlying_.allocator.New();
+  }
+  void deallocate(pointer p, size_type n) {
+    CHECK_CONDITION(n == 1);
+    underlying_.allocator.Delete(p);
+  }
+
+ private:
+  struct Storage {
+    explicit Storage(base::LinkerInitialized x) {}
+    PageHeapAllocator<T> allocator;
+    bool initialized;
+  };
+  static Storage underlying_;
+};
+
+template<typename T, class LockingTag>
+typename STLPageHeapAllocator<T, LockingTag>::Storage STLPageHeapAllocator<T, LockingTag>::underlying_(base::LINKER_INITIALIZED);
+
 }  // namespace tcmalloc
 
 #endif  // TCMALLOC_PAGE_HEAP_ALLOCATOR_H_
diff --git a/src/span.h b/src/span.h
index 83feda1..4e343ba 100644
--- a/src/span.h
+++ b/src/span.h
@@ -37,17 +37,63 @@
 #define TCMALLOC_SPAN_H_
 
 #include <config.h>
+#include <set>
 #include "common.h"
+#include "base/logging.h"
+#include "page_heap_allocator.h"
 
 namespace tcmalloc {
 
+struct SpanBestFitLess;
+struct Span;
+
+// Store a pointer to a span along with a cached copy of its length.
+// These are used as set elements to improve the performance of
+// comparisons during tree traversal: the lengths are inline with the
+// tree nodes and thus avoid expensive cache misses to dereference
+// the actual Span objects in most cases.
+struct SpanPtrWithLength {
+  explicit SpanPtrWithLength(Span* s);
+
+  Span* span;
+  Length length;
+};
+typedef std::set<SpanPtrWithLength, SpanBestFitLess, STLPageHeapAllocator<Span*, void> > SpanSet;
+
+// Comparator for best-fit search, with address order as a tie-breaker.
+struct SpanBestFitLess {
+  bool operator()(SpanPtrWithLength a, SpanPtrWithLength b);
+};
+
+// Wrapper which stores a SpanSet::iterator as a POD type.
+// This allows the iterator to be stored in a union below.
+struct SpanSetRevPtr {
+  char data[sizeof(SpanSet::iterator)];
+
+  SpanSet::iterator get_iterator() {
+    SpanSet::iterator ret;
+    memcpy(&ret, this, sizeof(ret));
+    return ret;
+  }
+
+  void set_iterator(const SpanSet::iterator& val) {
+    new (this) SpanSet::iterator(val);
+  }
+};
+
 // Information kept for a span (a contiguous run of pages).
 struct Span {
+  Span() {}
   PageID        start;          // Starting page number
   Length        length;         // Number of pages in span
   Span*         next;           // Used when in link list
   Span*         prev;           // Used when in link list
-  void*         objects;        // Linked list of free objects
+  union {
+    void*         objects;      // Linked list of free objects
+    SpanSetRevPtr rev_ptr;      // "pointer" (std::set iterator) to
+                                // SpanSet entry associated with this
+                                // Span.
+  };
   unsigned int  refcount : 16;  // Number of non-free objects
   unsigned int  sizeclass : 8;  // Size-class for small objects (or 0)
   unsigned int  location : 2;   // Is the span on a freelist, and if so, which?
@@ -71,6 +117,19 @@ void Event(Span* span, char op, int v = 0);
 #define Event(s,o,v) ((void) 0)
 #endif
 
+inline SpanPtrWithLength::SpanPtrWithLength(Span* s)
+    : span(s),
+      length(s->length) {
+}
+
+inline bool SpanBestFitLess::operator()(SpanPtrWithLength a, SpanPtrWithLength b) {
+  if (a.length < b.length)
+    return true;
+  if (a.length > b.length)
+    return false;
+  return a.span->start < b.span->start;
+}
+
 // Allocator/deallocator for spans
 Span* NewSpan(PageID p, Length len);
 void DeleteSpan(Span* span);
diff --git a/src/tests/tcmalloc_unittest.cc b/src/tests/tcmalloc_unittest.cc
index 25b2e41..a9c6429 100644
--- a/src/tests/tcmalloc_unittest.cc
+++ b/src/tests/tcmalloc_unittest.cc
@@ -1422,7 +1422,7 @@ static int RunAllTests(int argc, char** argv) {
 
     // Try strdup(), which the system allocates but we must free.  If
     // all goes well, libc will use our malloc!
-    p2 = strdup("test");
+    p2 = strdup("in memory of James Golick");
     CHECK(p2 != NULL);
     VerifyNewHookWasCalled();
     free(p2);
-- 
1.7.1

